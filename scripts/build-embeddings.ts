/**
 * RAG Chatbot Build Pipeline - Phase 1: Content Discovery
 *
 * Based on: /docs/rag-chatbot-implementation-plan.md v1.6
 *
 * This script implements the build-time pipeline for generating
 * embeddings and artifacts for the database-less RAG chatbot.
 *
 * Architecture: Uses filesystem-based content loading (gray-matter)
 * instead of astro:content virtual module for standalone execution.
 */

// Load environment variables from .env.local
import dotenv from 'dotenv';
dotenv.config({ path: '.env.local' });

import { discoverContent } from './content-loader.js';
import { chunkAll } from '../src/utils/chunking.js';
import type { ContentItem, Chunk, EmbeddingResult, ChunkMetadata } from '../src/types/chatbot.js';
import {
  EMBEDDING_CONFIG,
  CHUNKING_CONFIG,
  CACHE_CONFIG,
  ARTIFACT_VERSION,
  getTotalTokens,
  getAverageTokens,
} from '../src/types/chatbot.js';
import { pipeline } from '@huggingface/transformers';
import { setFloat16 } from '@petamoriken/float16';
import { createHash } from 'crypto';
import { put } from '@vercel/blob';
import { writeFile } from 'fs/promises';

// ============================================================================
// Phase 1: Content Discovery - Now handled by content-loader.ts
// ============================================================================
// Content loading moved to filesystem-based approach in content-loader.ts
// because astro:content is a Vite virtual module only available within
// Astro's build context. See scripts/content-loader.ts for implementation.

// ============================================================================
// Phase 3: Embedding Generation
// ============================================================================

/**
 * Generates embeddings for all chunks using Transformers.js
 *
 * Model: Xenova/all-MiniLM-L6-v2 (384 dimensions)
 * Process (spec lines 288-331):
 * 1. Load feature-extraction pipeline with quantization
 * 2. Process chunks in batches (batch size 32)
 * 3. Generate embeddings with mean pooling
 * 4. L2-normalize for cosine similarity via dot product
 * 5. Extract Float32 arrays (384 dimensions)
 *
 * @param chunks - Array of chunks from Phase 2
 * @returns Array of embedding results with chunk IDs
 */
async function generateEmbeddings(chunks: Chunk[]): Promise<EmbeddingResult[]> {
  console.log('üß† Phase 3: Embedding Generation');
  console.log(`   Loading model: ${EMBEDDING_CONFIG.model}`);

  // Load model (Node.js environment with quantization)
  const extractor = await pipeline(
    'feature-extraction',
    EMBEDDING_CONFIG.model,
    { dtype: 'q8' } // Use quantized model for faster inference (v3.x API)
  );

  console.log('   ‚úì Model loaded');

  const results: EmbeddingResult[] = [];

  console.log(`   Processing ${chunks.length} chunks in batches of ${EMBEDDING_CONFIG.batchSize}...`);

  // Process in batches for efficiency
  for (let i = 0; i < chunks.length; i += EMBEDDING_CONFIG.batchSize) {
    const batch = chunks.slice(i, i + EMBEDDING_CONFIG.batchSize);
    const texts = batch.map(c => c.text);

    // Generate embeddings with L2 normalization
    const embeddings = await extractor(texts, {
      pooling: 'mean',
      normalize: true  // L2-normalize for cosine similarity
    });

    // Extract Float32 arrays (model outputs FP32)
    for (let j = 0; j < batch.length; j++) {
      const embedding = Array.from(embeddings.data.slice(
        j * EMBEDDING_CONFIG.dimensions,
        (j + 1) * EMBEDDING_CONFIG.dimensions
      )) as number[];

      results.push({
        chunkId: batch[j].id,
        embedding: embedding, // FP32 from model
        dimensions: EMBEDDING_CONFIG.dimensions
      });
    }

    console.log(`   Processed ${Math.min(i + EMBEDDING_CONFIG.batchSize, chunks.length)}/${chunks.length} chunks`);
  }

  console.log('   ‚úì All embeddings generated');
  console.log(`   Total embeddings: ${results.length}`);
  console.log(`   Dimensions: ${results[0]?.dimensions || 0}`);
  console.log(`   Precision: FP32 (L2-normalized)\n`);

  return results;
}

// ============================================================================
// Phase 4: Serialization
// ============================================================================

/**
 * ArtifactManifest contains metadata for the RAG chatbot artifacts
 *
 * Spec: docs/rag-chatbot-implementation-plan.md - Phase 4, lines 395-421
 */
interface ArtifactManifest {
  version: string;
  buildTime: string;
  buildHash: string;
  model: {
    name: string;
    dimensions: number;
    normalization: string;
  };
  storage: {
    precision: string;
    accumulationPrecision: string;
  };
  chunks: {
    id: string;
    parentId: string;
    tokens: number;
    metadata: ChunkMetadata;
    embeddingOffset: number;
  }[];
  stats: {
    totalChunks: number;
    totalTokens: number;
    avgTokensPerChunk: number;
  };
}

/**
 * SerializedArtifacts represents the three output files
 *
 * Spec: docs/rag-chatbot-implementation-plan.md - Phase 4, lines 423-427
 */
interface SerializedArtifacts {
  embeddingsBuffer: ArrayBuffer;  // FP16 binary embeddings
  chunkTextBuffer: ArrayBuffer;   // Length-prefixed chunk text
  manifest: ArtifactManifest;     // Metadata JSON
}

/**
 * Computes SHA-256 hash of chunks and metadata for cache invalidation
 *
 * Hashes chunk content, IDs, metadata, and chunking config to ensure
 * hash changes when any of these change (not just text content).
 *
 * @param chunks - Array of chunks to hash
 * @returns Hex-encoded SHA-256 hash (first 16 chars)
 */
function computeContentHash(chunks: Chunk[]): string {
  // Include everything that should invalidate the cache:
  // 1. Chunk text content
  // 2. Chunk IDs (detects slug changes, reordering)
  // 3. Metadata (title, tags, sections, URLs)
  // 4. Chunking config (detects strategy changes)
  const hashInput = JSON.stringify({
    chunks: chunks.map(c => ({
      id: c.id,
      text: c.text,
      tokens: c.tokens,
      metadata: c.metadata
    })),
    config: CHUNKING_CONFIG,
    version: ARTIFACT_VERSION
  });

  const hash = createHash('sha256').update(hashInput).digest('hex');
  return hash.substring(0, 16); // First 16 chars for filename
}

/**
 * Serializes embeddings and chunks to binary artifacts
 *
 * Process (spec lines 355-428):
 * 1. Convert FP32 embeddings to FP16 binary (50% size reduction)
 * 2. Serialize chunk text to binary (length-prefixed strings)
 * 3. Create manifest with metadata (no chunk text)
 * 4. Return three artifacts ready for upload
 *
 * @param embeddings - FP32 embeddings from Phase 3
 * @param chunks - Chunks from Phase 2
 * @returns Serialized artifacts (embeddings.bin, chunks.bin, manifest.json)
 */
function serializeEmbeddings(
  embeddings: EmbeddingResult[],
  chunks: Chunk[]
): SerializedArtifacts {
  console.log('üíæ Phase 4: Serialization');

  // Step 1: Convert FP32 to FP16 binary
  console.log('   Converting embeddings: FP32 ‚Üí FP16');
  const buffer = new ArrayBuffer(embeddings.length * EMBEDDING_CONFIG.dimensions * 2); // 2 bytes per FP16
  const view = new DataView(buffer);

  let offset = 0;
  for (const embedding of embeddings) {
    for (let i = 0; i < EMBEDDING_CONFIG.dimensions; i++) {
      setFloat16(view, offset, embedding.embedding[i], true); // little-endian
      offset += 2;
    }
  }

  const embeddingsSizeKB = (buffer.byteLength / 1024).toFixed(2);
  console.log(`   ‚úì Embeddings: ${embeddingsSizeKB} KB (FP16)`);

  // Step 2: Serialize chunk text to binary (length-prefixed strings)
  console.log('   Serializing chunk text');
  const encoder = new TextEncoder();
  const chunkTextBuffers: Uint8Array[] = [];
  let totalChunkTextSize = 0;

  for (const chunk of chunks) {
    const textBytes = encoder.encode(chunk.text);
    const lengthBuffer = new Uint8Array(4);
    new DataView(lengthBuffer.buffer).setUint32(0, textBytes.length, true);

    chunkTextBuffers.push(lengthBuffer);
    chunkTextBuffers.push(textBytes);
    totalChunkTextSize += 4 + textBytes.length;
  }

  // Concatenate all chunk text buffers
  const chunkTextBuffer = new Uint8Array(totalChunkTextSize);
  let chunkTextOffset = 0;
  for (const buf of chunkTextBuffers) {
    chunkTextBuffer.set(buf, chunkTextOffset);
    chunkTextOffset += buf.length;
  }

  const chunkTextSizeKB = (chunkTextBuffer.byteLength / 1024).toFixed(2);
  console.log(`   ‚úì Chunk text: ${chunkTextSizeKB} KB (binary)`);

  // Step 3: Create manifest (metadata only, no chunk text)
  console.log('   Generating manifest');
  const buildHash = computeContentHash(chunks);

  const manifest: ArtifactManifest = {
    version: ARTIFACT_VERSION,
    buildTime: new Date().toISOString(),
    buildHash,
    model: {
      name: EMBEDDING_CONFIG.model,
      dimensions: EMBEDDING_CONFIG.dimensions,
      normalization: EMBEDDING_CONFIG.normalization
    },
    storage: {
      precision: 'fp16',
      accumulationPrecision: 'float64' // JS accumulation is always float64
    },
    chunks: chunks.map((chunk, idx) => ({
      id: chunk.id,
      parentId: chunk.parentId,
      // text removed - stored separately in chunks.bin
      tokens: chunk.tokens,
      metadata: chunk.metadata,
      embeddingOffset: idx * EMBEDDING_CONFIG.dimensions
    })),
    stats: {
      totalChunks: chunks.length,
      totalTokens: getTotalTokens(chunks),
      avgTokensPerChunk: getAverageTokens(chunks)
    }
  };

  const manifestSizeKB = (JSON.stringify(manifest).length / 1024).toFixed(2);
  console.log(`   ‚úì Manifest: ${manifestSizeKB} KB (JSON)`);

  const totalSizeKB = (
    buffer.byteLength +
    chunkTextBuffer.byteLength +
    JSON.stringify(manifest).length
  ) / 1024;
  console.log(`   Total artifacts: ${totalSizeKB.toFixed(2)} KB`);
  console.log(`   Build hash: ${buildHash}\n`);

  return {
    embeddingsBuffer: buffer,
    chunkTextBuffer: chunkTextBuffer.buffer,
    manifest
  };
}

// ============================================================================
// Phase 5: Artifact Upload
// ============================================================================

/**
 * Artifact configuration written to runtime
 */
interface ArtifactConfig {
  embeddingsUrl: string;
  chunksUrl: string;
  manifestUrl: string;
  version: string;
  buildHash: string;
}

/**
 * Uploads artifacts to Vercel Blob Storage
 *
 * Process (spec lines 448-503):
 * 1. Upload embeddings.bin with build hash in filename
 * 2. Upload chunks.bin with build hash in filename
 * 3. Upload manifest.json with build hash in filename
 * 4. Write URLs to src/config/chatbot-artifacts.json for runtime
 * 5. Set long cache headers (1 year) - immutable files
 *
 * @param artifacts - Serialized artifacts from Phase 4
 * @returns Artifact URLs configuration
 */
async function uploadArtifacts(artifacts: SerializedArtifacts): Promise<ArtifactConfig> {
  console.log('‚òÅÔ∏è  Phase 5: Artifact Upload');

  const buildHash = artifacts.manifest.buildHash;

  // Check for BLOB_READ_WRITE_TOKEN
  if (!process.env.BLOB_READ_WRITE_TOKEN) {
    throw new Error(
      'BLOB_READ_WRITE_TOKEN environment variable is required.\n' +
      'Set it in .env.local for local development or in Vercel project settings.'
    );
  }

  console.log(`   Uploading to Vercel Blob (hash: ${buildHash})`);

  // Upload embeddings.bin
  console.log('   Uploading embeddings.bin...');
  const embeddingsBlob = await put(
    `chatbot/embeddings-${buildHash}.bin`,
    artifacts.embeddingsBuffer,
    {
      access: 'public',
      contentType: 'application/octet-stream',
      addRandomSuffix: false,
      cacheControlMaxAge: CACHE_CONFIG.maxAgeSeconds // 1 year (immutable)
    }
  );
  console.log(`   ‚úì Embeddings: ${embeddingsBlob.url}`);

  // Upload chunks.bin
  console.log('   Uploading chunks.bin...');
  const chunksBlob = await put(
    `chatbot/chunks-${buildHash}.bin`,
    artifacts.chunkTextBuffer,
    {
      access: 'public',
      contentType: 'application/octet-stream',
      addRandomSuffix: false,
      cacheControlMaxAge: CACHE_CONFIG.maxAgeSeconds
    }
  );
  console.log(`   ‚úì Chunks: ${chunksBlob.url}`);

  // Upload manifest.json
  console.log('   Uploading manifest.json...');
  const manifestBlob = await put(
    `chatbot/manifest-${buildHash}.json`,
    JSON.stringify(artifacts.manifest, null, 2),
    {
      access: 'public',
      contentType: 'application/json',
      addRandomSuffix: false,
      cacheControlMaxAge: CACHE_CONFIG.maxAgeSeconds
    }
  );
  console.log(`   ‚úì Manifest: ${manifestBlob.url}`);

  // Write URLs to local config for runtime
  const config: ArtifactConfig = {
    embeddingsUrl: embeddingsBlob.url,
    chunksUrl: chunksBlob.url,
    manifestUrl: manifestBlob.url,
    version: artifacts.manifest.version,
    buildHash: buildHash
  };

  console.log('   Writing config to src/config/chatbot-artifacts.json...');
  await writeFile(
    'src/config/chatbot-artifacts.json',
    JSON.stringify(config, null, 2) + '\n'
  );
  console.log('   ‚úì Config written\n');

  return config;
}

// ============================================================================
// Main Execution
// ============================================================================

async function main() {
  try {
    console.log('ü§ñ RAG Chatbot Build Pipeline');
    console.log('   Implementation Plan v1.7 (DRY Compliant)\n');

    // Phase 1: Content Discovery
    const contentItems = await discoverContent();

    console.log('‚úì Phase 1 Complete');
    console.log(`  Total items: ${contentItems.length}\n`);

    // Phase 2: Chunking
    console.log('üìù Phase 2: Chunking');
    const chunks = chunkAll(contentItems);

    console.log(`   Created ${chunks.length} chunks`);
    console.log(`   Tokens: ${getTotalTokens(chunks)} total`);
    console.log(`   Average: ${Math.round(getAverageTokens(chunks))} tokens/chunk`);

    console.log('‚úì Phase 2 Complete');
    console.log(`  Total chunks: ${chunks.length}\n`);

    // Validation: Check for empty chunks
    if (chunks.length === 0) {
      throw new Error('No chunks generated! Check content discovery and chunking logic.');
    }

    // Phase 3: Embedding Generation
    const embeddings = await generateEmbeddings(chunks);

    console.log('‚úì Phase 3 Complete');
    console.log(`  Total embeddings: ${embeddings.length}\n`);

    // Phase 4: Serialization
    const artifacts = serializeEmbeddings(embeddings, chunks);

    console.log('‚úì Phase 4 Complete');
    console.log(`  Artifacts ready: embeddings.bin, chunks.bin, manifest.json\n`);

    // Phase 5: Artifact Upload
    const config = await uploadArtifacts(artifacts);

    console.log('‚úì Phase 5 Complete');
    console.log(`  Build hash: ${config.buildHash}`);
    console.log(`  Artifacts uploaded to Vercel Blob\n`);

    // TODO: Phase 6 - Manifest Generation

  } catch (error) {
    console.error('‚ùå Build failed:', error);
    process.exit(1);
  }
}

// Run if executed directly
if (import.meta.url === `file://${process.argv[1]}`) {
  main();
}
